\section{Active Appearance Model (AAM)}
\label{sec:bg_aam}
An Active Appearance Model (AAM) is a combination of linear statistical shape and appearance model with corresponding fitting algorithms \cite{Matthews2004, Cootes2001}. AAM separate that model in two parts, shape model and appearance model. 

\subsection{Shape Model}
Shapes are defined as vertexes of a mesh. A shape model is a function that can express arbitrary shapes by using a base shape plus a linear combination of shape vectors. Mathematical definition of shape model shown below:
\begin{equation}
s=s_0+\sum^n_{i=1}p_i s_i
\end{equation}
where shape $s$ is expressed by base shape $s_0$ plus linear combination of shape vectors $s_i$, each having coefficient $p_i$. Note that $s_i$ need to be orthonormal. In general, shape model are generated by applying Principle Component Analysis (PCA). Therefore $s_0$ is the mean shape with $s_i$ are eigenvectors(principle components).

\subsection{Appearance Model}
\label{sec:appearance_model}
Appearances are defined as pixels lie inside base shape $s0$. Similar to shape models, appearance models are defined in same format, a base appearance plus linear combination of appearance vectors. Mathematical definition of appearance model is expressed below:
\begin{equation}
A(x)=A_0(x)+\sum^m_{i=1}\lambda_iA_i(x)
\end{equation}
where $x \in s0$, appearance $A(x)$ is expressed by base appearance $A_0(x)$ plus linear combination of appearance vectors $A_i(x)$, each having coefficient $\lambda_i$. Note that $A_i(x)$ need to be orthonormal. In general, appearance model are generated by applying Principle Component Analysis (PCA). Therefore $A_0(x)$ is the mean appearance with $A_i(x)$ are eigenvectors(principle components).

\subsection{AAM Instance}
Active Appearance Model combines shape model and appearance model to generate instances. Instance generating procedure is starting with compute warping from base shape $s_0$ to instance shape $s$, to be more specific, piecewise affine warp in general. The warp is required to map appearance instance $A(x)$ to shape instance $s$ since appearance instance is built on base shape $s_0$. The way piecewise affine works is finding corresponding triangles in both base shape and instance shape, then for each pixels within a triangle on $s_0$, finding position of the pixel of same triangle in $s$. The final model instance should satisfies  following expression:
\begin{equation}
M(W(x;p))=A(x)
\end{equation}
where $M$ is AAM instance and $W(x;p)$ represent piecewise affine warp.

\subsection{AAM Fitting}
In the case of fitting an AAM to an input image $I(x)$, aam instance $M(W(x;p))=A(x)$ should be similar as $I(x)$. For any pixel $x$ in base shape $s_0$, the corresponding pixel in the input image is $W(x;p)$. Appearance of pixel $x$ are defined as $A(x)=A_0(x)+\sum^m_{i=1}\lambda_iA_i(x)$ mentioned in section \ref{sec:appearance_model}. For pixel $W(x;p)$, appearance of input image is expressed as $I(W(x;p))$. A formal fitting algorithm can now be defined: the objective is to discover optimal AAM instance that fit given input image maximumly. In other words, we want to minimise following error function to minimise difference between $A(x)$ and $I(W(x;p))$:
\begin{equation} 
\label{eq:aamfittingbase}
E(x)=||A_0(x)+\sum^m_{i=1}\lambda_iA_i(x)-I(W(x;p))||^2
\end{equation}
where input image is denoted as $I$ and $||.||^2$ is the L2 norms. To be more specific, image $I$ is warped back onto base shape $s_0$ before computing difference against AAM instance appearance.

\subsection{Alternating Minimisation}
As the expression revealed, there are two variables contained ($p$ and $\lambda$) so it is of importance to minimise them simultaneously. As there is no dependency between AAM shape and appearance, solving $p$ and $\lambda$ alternatively is plausible. Considering creating a linear subspace, which is spanned by collection of appearance component vector $A_i$, denoted as $span(A_i)$ and $span(A_i)^\perp$ as orthogonal complement subspace. Therefore \ref{eq:aamfittingbase} can be rewrite as:
\begin{equation} 
\label{eq:aamfittingappearancevariance}
E(x)=||A_0(x)+\sum^m_{i=1}\lambda_iA_i(x)-I(W(x;p))||^2_{span(A_i)^\perp}+||A_0(x)+\sum^m_{i=1}\lambda_iA_i(x)-I(W(x;p))||^2_{span(A_i)}
\end{equation}
where $||.||^2_L$ is L2 norm of vector projected into linear subspace $L$. Because norm considers only orthogonal components of subspace $L$ so any components lies in $L$ can be dropped. So the above function can be optimised as:
\begin{equation} 
\label{eq:aamfittingappearancevariancesubspace}
E(x)=||A_0(x)-I(W(x;p))||^2_{span(A_i)^\perp}+||A_0(x)+\sum^m_{i=1}\lambda_iA_i(x)-I(W(x;p))||^2_{span(A_i)}
\end{equation}
The first section contains only variable $p$, therefore the optimal value of the equation can be found by first minimising $||A_0(x)-I(W(x;p))||^2_{span(A_i)^\perp}$ with respect to $p$ alone before substitute minimal $p$ value in $||A_0(x)+\sum^m_{i=1}\lambda_iA_i(x)-I(W(x;p))||^2_{span(A_i)}$ then minimise with respect to $\lambda$. Mathematical equations shown below:
\begin{equation}
\label{eq:minimisep}
\operatorname*{arg\,min}_p=||A_0(x)-I(W(x;p))||^2_{span(A_i)^\perp}
\end{equation}
Based on the assumption of basis vector $A_i$ are orthonormal, $\lambda$ can be computed with closed-form solution:
\begin{equation}
\label{eq:minimiselambda}
\lambda_i=\sum_{x\in s_0}A_i(x).[I(W(x;p))-A_0(x)]
\end{equation}

\subsection{Image Alignment}
Minimising equation \ref{eq:minimisep} also referenced as Image Alignment. The purpose of image alignment is finding constant template image in given input image \cite{Matthews2004}. 

\subsubsection{Lucas-Kanada Algorithm}
Using gradient descent method for image alignment was first introduced by Lucas Kanada by minimising square error between two images which having same form as equation \ref{eq:minimisep}. Solving $p$ is a nonlinear process as pixel value of an image generally unrelated to pixel coordinate. Lucas Kanada make the problem linear by solving the problem with an incremental parameter $\delta p$ with assumption that initial value of $p$ is known. Thus the problem can be solved iteratively for $\delta p$. Equation \ref{eq:minimisep} can now be solved with respect to $\delta p$ as:
\begin{equation}
\label{eq:minimisedp}
||A_0(x)-I(W(x;p+\delta p))||^2
\end{equation}
where $p$ gets updated at every iteration by $p=p+\delta p$. A Taylor expansion on above function gives:
\begin{equation}
\label{eq:minimisedptaylor}
||A_0(x)-I(W(x;p))-\nabla I\frac{\partial W}{\partial p} \delta p))||^2
\end{equation}
where $\frac{\partial W}{\partial p}$ is the Jacobian and $\nabla I$ is image gradient. $\delta p$ can be solved in closed form:
\begin{equation}
\label{eq:dp}
\delta p=\bm{H}^{-1}\sum_x[\nabla I\frac{\partial W}{\partial p}]^T[A_0(x)-I(W(x;p))]
\end{equation}
where $\bm{H}$ is the Hessian matrix:
\begin{equation}
\label{eq:hessian}
\bm{H}=\sum_x[\nabla I\frac{\partial W}{\partial p}]^T[\nabla I\frac{\partial W}{\partial p}]
\end{equation}
The new variable $p$ can now be updated by $p=p+\delta p$ before involving in next iteration of optimisation. Iterations ends when converged or specific criteria reached. 

However, Lucas-Kanada Gaussian-Newton descent is slow due to computational intensity on Hessian Matrix $\bm{H}$, which contains image gradient $\nabla I$ and warp Jacobian $\frac{\partial W}{\partial p}$ and both depends on $p$ but $p$ is varying at every iteration. Besides, computation of $\nabla I$ $\frac{\partial W}{\partial p}$ also slow.

\subsubsection{Forwards Compositional Algorithm}
\label{sec:fca}
Instead of updating $p$ from estimated $\delta p$ iteratively, \cite{Matthews2004} mentioned compositional algorithms of updating warp transformation. The cost function are:
\begin{equation}
\label{eq:forwardcomposition}
||A_0(x)-I(W(W(x;\delta p);p))||^2
\end{equation}
where $W(x;\delta p)$ denotes incremental warp. The update step of new warp $W(x;p)$ is:
\begin{equation}
W(x;p) = W(x;p)\circ W(x;\delta p)
\end{equation}
Applying Taylor expansion on equation \ref{eq:forwardcomposition} gives:
\begin{equation}
\label{eq:forwardcompositiontaylor}
||A_0(x)-I(W(W(x;0);p))-\nabla I(W(x;p))\frac{\partial W}{\partial p}\delta p||^2
\end{equation}
where $W(x;0)$ is the identical warp that $W(x;0) = x$. Comparing with equation \ref{eq:minimisedptaylor}, image gradient is now computed for $I(W(x;p))$. The key improvement is that update of warp is evaluation with respect to $p=0$ therefore Jacobian is now computed at $(x;0)$. Thus a constant Jacobian through all iterations so can be precomputed.

\subsubsection{Inverse Compositional Algorithm}
Inverse Compositional Algorithm are based on Forwards Compositional Algorithm that the purpose of template image $A_0(x)$ and input image $I$ are inversed to further improve performance by precomputing $\delta p$ and Hessian matrix \cite{Matthews2004}. Cost function for inverse compositional algorithm are:
\begin{equation}
\label{eq:inversecomposition}
||I(W(x;p))-A_0(W(x;\delta p))||^2
\end{equation}
where $W(x;\delta p)$ denotes incremental warp on template image. So the update step of new warp $W(x;p)$ is:
\begin{equation}
W(x;p) = W(x;p)\circ W(x;\delta p)^{-1}
\end{equation}
Applying Taylor expansion on equation \ref{eq:inversecomposition} gives:
\begin{equation}
\label{eq:inversecompositiontaylor}
||I(W(x;p))-A_0(W(x;\delta p))-\nabla A_0\frac{\partial W}{\partial p}\delta p||^2
\end{equation}
where $W(x;0)$ is the identical warp that $W(x;0) = x$. The closed form solution of $\delta p$ is:
\begin{equation}
\label{eq:dpic}
\delta p=\bm{H}^{-1}\sum_x[\nabla A_0\frac{\partial W}{\partial p}]^T[I(W(x;p))-A_0(x)]
\end{equation}
where $\bm{H}$ is the Hessian matrix:
\begin{equation}
\label{eq:hessianic}
\bm{H}=\sum_x[\nabla A_0\frac{\partial W}{\partial p}]^T[\nabla A_0\frac{\partial W}{\partial p}]
\end{equation}
As the equation states, image template $A_0$ is constant and $\frac{\partial W}{\partial p}$ is evaluated at $p=0$ same as the Jacobian part in forward compositional algorithm. Therefore both $\delta p$ and Hessian $\bm{H}$ can be precomputed before optimising cost function. 

\subsection{Pseudo Algorithms}

The pseudo code of AAM fitting using inverse compositional algorithm is shown in Algorithm \ref{alg:aam}\cite{Matthews2004}:

\begin{algorithm}[ht]
\caption{Fitting Active Appearance Model with Inverse Compositional Algorithm}
\label{alg:aam}
Compute gradient $\nabla A_0$ of image template $A_0(x)$\;
Compute Jacobian $\frac{\partial W}{\partial p}$ at $(x;0)$\;
Compute Hessian matrix $H$\;
\While{not converged or criteria not met}{
    Warp input image $I$ with transform $W(x;p)$\;
    Compute error image $I(W(x;p))-A_0(x)$\;
    Compute $\delta p$ by multiplying inverse Hessian matrix $H^{-1}$\;
    Update warp by $W(x;p) = W(x;p)\circ W(x;\delta p)^{-1}$\;
    Update $\lambda$ by $\lambda_i=\sum_{x\in s_0}A_i(x).[I(W(x;p))-A_0(x)]$\;
}  
\end{algorithm}


% AAM is trained using a set of images where each image contains clear annotated images using same number of landmarks. Shape model is trained using landmark coordinates before applying PCA on shapes to produce a mean shape with some eigenvectors to control the deformation of mean shape. For training appearance model, training images are warped to a reference frame, which contains landmarks of mean shape, so we have different objects with same shape but different texture. Then PCA is used again applying on the warped images to generate mean appearance with eigenvectors controlling variations of texture. Fitting an AAM model to a new image is using Lucas Kanade Alternating Inverse Composition algorithm which is clearly explained in references \cite{Matthews}.