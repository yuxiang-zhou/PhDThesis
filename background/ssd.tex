\section{Supervised Descent Methods (SDM)}
\label{sec:SSD}
As section \ref{sec:bg_aam} briefly discussed, Newton's descent method played an important role in optimising non-linear cost functions. It is widely used as the second order descent methods approach are considered robust, fast and reliable for nonlinear optimization~\cite{Xiong2013}. However few major disadvantages affect second order descent methods: (1) Function might not be analytical differentiable. (2) Hessian might be computational intensive to calculate. (3) Hessian might not be positive semidefinite, in other word the function might not be convex. (4) Functions required to be twice-differentiable.

Supervised Descent Method (SDM) are used to replace Gauss-Newton optimization. SDM proposed the idea that descent method directions can be supervisory trained, which leads to significantly reduce in computational cost since calculation of Jacobian, especially Hessian matrices is avoid. In training phase, supervised descent method learns a set of descent directions that minimise mean of nonlinear least square error functions at sampled points. In testing phase, the error function are optimised with pre-trained descent directions without computing Jacobian or Hessian.

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.8\textwidth]{figures/SDM_Overview.png}
% \caption{Training procedure of SDM.}
% \label{fig:smd_training}
% \end{figure}

\subsection{SDM Algorithms}
\label{sec:SDMs}
For any given image $d \in R^{m}$ having $m$ pixels with corresponding $p$ landmarks $d(x) \in R^{p}$, feature extracted image at lanmark points is defined as $h(d(x)) \in R^{128p}$. Features e.g. Histograms of Oriented Gradients (HoGs) and Scale Invariant Feature Transform (SIFT)~\cite{Dalal2005,SIFT} are chosen due to they are robust representations agains noise arnd artifacts. Generally, an image alignment algorithm are defined~\cite{Xiong2013}:
\begin{equation}
\label{eq:ssdnd}
f(x_{k+1})=||h(d(x_k+\delta x))-\Phi_*||^2_2
\end{equation}
where at initialisation $x_0$ represents initial representation of landmarks which generated by simple object detector, $x_*$ is ground truth manual annotated landmarks, $x_k$ will be updated at the end of each iteration by $x_k+1=x_k + \delta x$ and $\Phi_* = h(d(x_*))$ represent feature extracted training image. 

However, with Newton's descent methods, calculate $\delta x$ requires computation of Jacobian and Hessian. A Taylor expansion of equation \ref{eq:ssdnd} are:
\begin{equation}
\label{eq:ssdndtaylor}
f(x_{k+1})=f(x_k)+J_f(x_k)^T\delta x+\frac{1}{2}\delta x^TH(x_k)\delta x
\end{equation}
where Jacobian $J_f{x_k}$ and Hessian $H(x_k)$ matrix are of $f$ evaluated at $x_k$. $\delta x_k$ has closed-form solution:
\begin{align}
\label{eq:ssddx}
R_{k+1}=\delta x_{k+1} = -H^{-1}J_f = -2H_{-1}J^T_h(\delta \Phi_k) \\
x_{k+1}=x_k+\delta x_k = x_{k}-2H_{-1}J^T_h(\delta \Phi_k)
\end{align}
where $J_f$ can be shown equivalent as $J^T_h(\delta \Phi_k)$, $\Phi_k=\Phi_k-\Phi_*$, $\Phi_k=h(d(x_k))$ and $R_k$ is referred as descent direction. In the above solution, $\Phi$ needs to be twice-differentiable means feature extractor $h$ has to have $2^{nd}$ order derivative, which is normally achieved with numerical approximation that generally involves heavy computation.

Supervised Descent Method introduces methods to train descent directions $R_k$ to avoid evaluating Jacobian and Hessian thus improves performance and removes criteria of functions are twice-differentiable. With SDM, equation \ref{eq:ssddx} is rewritten as:
\begin{align}
\label{eq:ssdrx}
R_{k+1}=\delta x_{k+1} = R_k\Phi_k + b_k \\
x_{k+1}=x_k+\delta x_k = x_{k}+R_k\Phi_k + b_k
\end{align}
where SDM learns a sequence of $[R_0,\dotsc,R_k,\dotsc]$ and $[b_0,\dotsc,b_k,\dotsc]$ so the iterative computation of $x_k$ converges to $x_*$.

\subsection{SDM Training}
Suppose training images and corresponding ground-truth landmarks are defined as $[d^1,\dotsc,d^i,\dotsc]$ and $[x^1_*,\dotsc,x^i_*,\dotsc]$ correspondingly. For each training image $d^i$, initial estimation of landmarks $x^i_0$ are predicted based on optimal landmark positions and related $R_0$ and $b_0$ can be obtain from optimising cost function from multiple $x^i_0$ and $x^i_*$. The sub-sequential $R_k$ and $b_k$ can be calculated as linear regression problem:
\begin{equation}
\operatorname*{arg\,min}_{R_k,b_k}\sum_{d^i}\sum_{x^i_k}||\delta x^{ki}_*-R_k\Phi^i_k-b_k||^2
\end{equation}
where $\delta x^{ki}_*=x^i_*-x^i_k$ that is used to update $x^{^i_k}$ and $\Phi^i_k$ for next iteration. Thus the program generates sequence of $[d^1,\dotsc,d^i,\dotsc]$ and $[x^1_*,\dotsc,x^i_*,\dotsc]$ for fitting without considering Jacobian and Hessian.